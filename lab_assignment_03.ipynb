{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meghanagpudi/INFO-5502-Meghana/blob/main/lab_assignment_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_dVrv_N4tsb"
      },
      "source": [
        "## The third Lab-assignment (07/22/2022 11:59'AM' - 07/26/2022 11:59PM, 50 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0BACzO64tsd"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3zTSO_o4tse"
      },
      "source": [
        "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTJxzLp14tsf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Domain problem:  To find out the job posts which has Data Scientist as job role.\n",
        "Research Question: To get movies data using the Times Job website\n",
        "In this we get list of job posts, hence we can understand current requirement of jobs.\n",
        "\n",
        "\n",
        "The attributes present are:\n",
        ". compny_name\n",
        ". skillset\n",
        ". Published_time\n",
        "\n",
        "\n",
        "The data is retrived from the link:\n",
        "https://www.timesjobs.com\n",
        "\n",
        "\n",
        "The steps that are involved in the process to collect and store data:\n",
        "1. We use web scraping for scraping the data invloved, by using BeautifulSoup library in the Times job website that contains several pages\n",
        "2. Every page is scraped and the information required is retrived.\n",
        "3. We get the compny_name, skillset and Published_time details after we inspect the page on click on its HTML script\n",
        "4. We then retrive the data by parsing through the details present and then copy them to csv format.\n",
        "5. Finally, we get the data in the xlsx format after performing the web scrap.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2XvIFPL4tsg"
      },
      "source": [
        "Question 2 (30 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u0LgZs34tsh"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "Arr=[]\n",
        "for i in range(0,10): \n",
        "    html_Text=requests.get('https://www.timesjobs.com/candidate/job-search.html?from=submit&actualTxtKeywords=Data%20Scientist&searchBy=0&rdoOperator=OR&searchType=personalizedSearch&luceneResultSize=1000&postWeek=60&txtKeywords=data%20scientist&pDate=I&sequence='+str(i)+'&startPage=1').text\n",
        "    soup=bs(html_Text,'lxml')  #parsing html using BeautifulSoup and lxml\n",
        "    job=soup.find_all('li',class_='clearfix job-bx wht-shd-bx')\n",
        "#print(job)\n",
        "for i in job:\n",
        "    compny_name=soup.find('h3',class_='joblist-comp-name').text.replace(' ','')\n",
        "    skillset=soup.find('span',class_='srp-skills').text.replace(' ','')\n",
        "    Pblshed_time=soup.find('span',class_='sim-posted').text\n",
        "    Arr.append([compny_name,skillset,Pblshed_time])\n",
        "#print(soup.find('ul',class_='top-jd-dtl clearfix').text)\n",
        "df=pd.DataFrame(Arr,columns=['compny_name','skillset','Pblshed_time']) \n",
        "print(\"Length of DataFrame:\",len(df))\n",
        "df.to_csv('DataScientist_job.csv',index= False ,header = False)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acOsDATG4tsh"
      },
      "source": [
        "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cLOHwgO4tsh"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "'''\n",
        "\n",
        "Please write you answer here:\n",
        "'''\n",
        "\n",
        "Using the SuperMarket sales data from Kaggle.\n",
        "Dataset link : https://www.kaggle.com/aungpyaeap/supermarket-sales\n",
        "The dataset contains attributes date, Branch, Cogs and others\n",
        "\n",
        "Steps invloved in cleaning the data:\n",
        "1. Converting the attribute dates in appropriate format.\n",
        "2. In the dataframe removing all the unnecessary rows.\n",
        "3. Then, the dataframe index is changed.\n",
        "4. Converting and cleaning the columns which are numericals to Strings using .str() function.\n",
        "5. In the data frame clean the entire dataset element wise by using applymap()\n",
        "6. To skip the rows that aren't necessary and removing all the stop words from the csv data\n",
        "7. Then, removing any spaces that aren't required and also removing special characters.\n",
        "8. To check if any Null or blank data is present.\n",
        "9. If any imbalanced data present, we balance it by using SMOTE technique which performs over and under sampling.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c6a7f839effdddf35473dacbb9a8184e57ebbba5133cab03b12e4f28d4f0d0f"
      }
    },
    "colab": {
      "name": "lab_assignment_03.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}